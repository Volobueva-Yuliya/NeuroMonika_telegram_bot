{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "T2ixUtiUKszJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import namedtuple\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vyu0XJiJdhtR",
    "outputId": "e918d5b2-69d7-4598-ec7d-fd7116d9cdc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIkNKE6uVjvX"
   },
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==1.13.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDD99Snk3WmJ"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVq7AM81SWe6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "T4gzUUHmR_gD"
   },
   "outputs": [],
   "source": [
    "def load_book(path):\n",
    "    \"\"\"Load a book from its file\"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        book = f.read()\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "lZnMPogVSCDx"
   },
   "outputs": [],
   "source": [
    "# Считывание имен файлов\n",
    "path = './books/'\n",
    "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "book_files = book_files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "4I070kvqSEG9"
   },
   "outputs": [],
   "source": [
    "# Загрузка книг, используя имена файлов\n",
    "books = []\n",
    "for book in book_files:\n",
    "    books.append(load_book(path+book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnHPRQwESGXY",
    "outputId": "8fd8efb3-62cb-4622-cfdd-4d8539d5542d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 154896 words in ayvengo.txt.\n",
      "There are 197822 words in bednaya-liza.txt.\n",
      "There are 17569 words in belye-nochi.txt.\n",
      "There are 302988 words in bratya-karamazovy.txt.\n",
      "There are 26753 words in evgeniy-onegin.txt.\n",
      "There are 14569 words in granatovyy-braslet.txt.\n",
      "There are 31467 words in kapitanskaya-dochka.txt.\n",
      "There are 387018 words in graf-monte-kristo.txt.\n",
      "There are 6081 words in kashtanka.txt.\n",
      "There are 16212 words in nedorosl.txt.\n",
      "There are 59921 words in otcy-i-deti.txt.\n",
      "There are 220409 words in popal-tak-popal-geksalogiya-si.txt.\n",
      "There are 177026 words in prestuplenie-i-nakazanie-dr-izd.txt.\n",
      "There are 90788 words in puteshestviya-dushi.txt.\n",
      "There are 32080 words in reyder-si.txt.\n",
      "There are 40708 words in zachem-mne-lyubov-si.txt.\n"
     ]
    }
   ],
   "source": [
    "# Анализ количества слов в книгах/словарях\n",
    "for i in range(len(books)):\n",
    "    print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jZxvvmJ6WC3",
    "outputId": "574d0e98-dff1-46d8-968c-ec2cc9762bba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "ksUUr8z5U9Pk",
    "outputId": "8b7ea4ef-1545-4ced-814d-5fe3768002d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Мировую славу Вальтеру Скотту – поэту, переводчику, историку, романисту – принесли его исторические романы.\\n\\nВ одном из самых известных его романов – «Айвенго» бушуют феодальные междоусобицы, льется кровь в крестовых походах. Молодому рыцарю Уильфреду Айвенго приходится копьем и мечом отстаивать свою честь и права, свою возлюбленную – прекрасную леди Ровену, руки которой добивается подлый злодей – крестоносец Бриан де Буагильбер…\\n\\n\\n\\n\\n\\n* * *\\n\\n\\n\\nВальтер СкоттПредисловие\\n\\nПосвящение достопочтенному'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка считанного текста\n",
    "books[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "48jQxo486c89"
   },
   "outputs": [],
   "source": [
    "# Удаление ненужных символов и лишних пробелов из текста\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n', ' . ', text) \n",
    "    text = re.sub('\\s',' . ', text)\n",
    "    text = re.sub('[^А-Яа-я\\. ]', ' ', text)\n",
    "    text = re.sub(' \\.',' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "jYiEQWbz7G5n"
   },
   "outputs": [],
   "source": [
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_text(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLk2mDit7JAo",
    "outputId": "ef4556df-999a-4e4f-9564-7ec76cde8da8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "wIiMwatEVDTQ",
    "outputId": "44e00511-55b8-4b2a-fa17-b831fa0f5cab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Мировую славу Вальтеру Скотту поэту переводчику историку романисту принесли его исторические романы. В одном из самых известных его романов Айвенго бушуют феодальные междоусобицы льется кровь в крестовых походах. Молодому рыцарю Уильфреду Айвенго приходится копьем и мечом отстаивать свою честь и права свою возлюбленную прекрасную леди Ровену руки которой добивается подлый злодей крестоносец Бриан де Буагильбер Вальтер СкоттПредисловие Посвящение достопочтенному д ру Драйездасту Ф. А. С. в Касл Г'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка очищенного текста\n",
    "clean_books[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "4DPkiFX87LSx"
   },
   "outputs": [],
   "source": [
    "# Создание словаря для преобразования словаря (символов) в целые числа\n",
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1\n",
    "# Добавление маркеров\n",
    "codes = ['<PAD>','<EOS>','<GO>']\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBWvPwB76dv",
    "outputId": "a3ded8fc-2b27-41fa-fe91-7a9a0b42d618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 69 characters.\n",
      "[' ', '.', '<EOS>', '<GO>', '<PAD>', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я']\n"
     ]
    }
   ],
   "source": [
    "# Проверка словаря\n",
    "vocab_size = len(vocab_to_int)\n",
    "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
    "print(sorted(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RQ1_288CO2R",
    "outputId": "e6b7c378-e80b-4d72-d99a-e30669e530b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['а']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "nO_gi4Fz78XJ"
   },
   "outputs": [],
   "source": [
    "# Создание другой словарь для преобразования целых чисел в соответствующие им символы\n",
    "int_to_vocab = {}\n",
    "for character, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSEB4FcI8Bo6",
    "outputId": "74ef67d3-f13d-4daf-f2c0-b1f12215d6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 102617 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Split the text from the books into sentences.\n",
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split('. '):\n",
    "        sentences.append(sentence + '.')\n",
    "print(\"There are {} sentences.\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bz5hkbYf8DxI",
    "outputId": "c31e7582-012e-4c85-9415-9179e4cda24a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Мировую славу Вальтеру Скотту поэту переводчику историку романисту принесли его исторические романы.',\n",
       " 'В одном из самых известных его романов Айвенго бушуют феодальные междоусобицы льется кровь в крестовых походах.',\n",
       " 'Молодому рыцарю Уильфреду Айвенго приходится копьем и мечом отстаивать свою честь и права свою возлюбленную прекрасную леди Ровену руки которой добивается подлый злодей крестоносец Бриан де Буагильбер Вальтер СкоттПредисловие Посвящение достопочтенному д ру Драйездасту Ф.',\n",
       " 'А.',\n",
       " 'С.',\n",
       " 'в Касл Гейт Йорк Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Глава Вальтер Скотт Айвенго Предисловие До сих пор автор Уэверли неизменно пользовался успехом у читателей и в избранной им области литературы мог по праву считаться баловнем судьбы.',\n",
       " 'Однако было ясно что слишком часто появляясь в печати он в конце концов должен был исчерпать благосклонность публики если бы не изобрел способа придать видимость новизны своим последующим произведениям.',\n",
       " 'Прежде для оживления повествования автор обращался к шотландским нравам шотландскому говору и шотландским характерам которые были ему ближе всего знакомы.',\n",
       " 'Но такая односторонность несомненно должна была привести его к некоторому однообразию и повторениям и заставила бы наконец читателя заговорить языком Эдвина из Повести Парнелла Кричит он Прекрати рассказ Уже довольно Хватит с нас Брось фокусы свои Нет ничего опаснее для репутации профессора изящных искусств если только в его возможностях избежать этого чем приклеенный к нему ярлык художника маньериста или предположение что он способен успешно творить лишь в одном и весьма узком плане.',\n",
       " 'Публика вообще склонна считать что художник заслуживший ее симпатии за какую нибудь одну своеобразную композицию именно благодаря своему дарованию не способен взяться за другие темы.',\n",
       " 'Публика недоброжелательно относится к тем кто ее развлекает когда они пробуют разнообразить используемые ими средства это проявляется в отрицательных суждениях высказываемых обычно по поводу актеров или художников которые осмелились испробовать свои силы в новой области для того чтобы расширить возможности своего искусства.',\n",
       " 'В этом мнении есть доля правды как и во всех общепринятых суждениях.',\n",
       " 'В театре часто бывает что актер обладающий всеми внешними данными необходимыми для совершенного исполнения комедийных ролей лишен из за этого возможности надеяться на успех в трагедии.',\n",
       " 'Равным образом и в живописи и в литературе художник или поэт часто владеет лишь определенными изобразительными средствами и способами передачи некоторых настроений что ограничивает их в выборе предметов для изображения.',\n",
       " 'Но гораздо чаще способности доставившие человеку популярность в одной области обеспечивают ему успех и в других.',\n",
       " 'Это в значительно большей степени относится к литературе чем к театру или живописи потому что здесь автор в своих исканиях не ограничен ни особенностями своих черт лица ни телосложением ни навыками в использовании кисти соответствующими лишь известному роду сюжетов.',\n",
       " 'Быть может эти рассуждения и неправильны но во всяком случае автор чувствовал что если бы он ограничился исключительно шотландскими темами он не только должен был бы надоесть читателям но и чрезвычайно сузил бы возможности которыми располагал для доставления им удовольствия.',\n",
       " 'В высокопросвещенной стране где столько талантов ежемесячно занято развлечением публики свежая тема на которую автору посчастливилось натолкнуться подобна источнику в пустыне Хваля судьбу в нем люди видят счастье.',\n",
       " 'Но когда люди лошади верблюды и рогатый скот замутят этот источник его вода становится противной тем кто только что пил ее с наслаждением а тот кому принадлежит заслуга открытия этого источника должен найти новые родники и тем самым обнаружить свой талант если он хочет сохранить уважение своих соотечественников.',\n",
       " 'Если писатель творчество которого ограничено кругом определенных тем пытается поддержать свою репутацию подновляя сюжеты которые уже доставили ему успех его без сомнения ожидает неудача.',\n",
       " 'Если руда еще не вся добыта то во всяком случае силы рудокопа неизбежно истощились.',\n",
       " 'Если он в точности подражает прозаическим произведениям которые прежде ему удавались он обречен удивляться тому что они больше не имеют успеха Если он пробует по новому подойти к прежним темам он скоро понимает что уже не может писать ясно естественно и изящно и вынужден прибегнуть к карикатурам для того чтобы добиться необходимого очарования новизны.',\n",
       " 'Таким образом желая избежать повторений он лишается естественности.',\n",
       " 'Быть может нет необходимости перечислять все причины по которым автор шотландских романов как их тогда называли попытался написать роман на английскую тему.',\n",
       " 'В то же время он намеревался сделать свой опыт возможно более полным представив публике задуманное им произведение как создание нового претендента на ее симпатии чтобы ни малейшая степень предубеждения будь то в пользу автора или против него не воспрепятствовала беспристрастной оценке нового романа автора Уэверли но впоследствии он оставил это намерение по причинам которые изложит позднее.',\n",
       " 'Автор избрал для описания эпоху царствования Ричарда это время богато героями имена которых способны привлечь общее внимание и вместе с тем отмечено резкими противоречиями между саксами возделывавшими землю и норманнами которые владели этой землей в качестве завоевателей и не желали ни смешиваться с побежденными ни признавать их людьми своей породы.',\n",
       " 'Мысль об этом противопоставлении была взята из трагедии талантливого и несчастного Логана Руннемед посвященной тому же периоду истории в этой пьесе автор увидел изображение вражды саксонских и норманских баронов.',\n",
       " 'Однако сколько помнится в этой трагедии не было сделано попытки противопоставить чувства и обычаи обоих этих племен к тому же было очевидно что изображение саксов как еще не истребленной воинственной и высокомерной знати было грубым насилием над исторической правдой.',\n",
       " 'Ведь саксы уцелели именно как простой народ правда некоторые старые саксонские роды обладали богатством и властью но их положение было исключительным по сравнению с униженным состоянием племени в целом.',\n",
       " 'Автору казалось что если бы он выполнил свою задачу читатель мог бы заинтересоваться изображением одновременного существования в одной стране двух племен побежденных отличавшихся простыми грубыми и прямыми нравами и духом вольности и победителей замечательных стремлением к воинской славе к личным подвигам ко всему что могло сделать их цветом рыцарства и эта картина дополненная изображением иных характеров свойственных тому времени и той же стране могла бы заинтересовать читателя своей пестротой если бы автор со своей стороны оказался на высоте положения.']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка корректности разделенного текста по предложениям\n",
    "sentences[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "zWNQ6H3C8IaX"
   },
   "outputs": [],
   "source": [
    "# Преобразование предложений в целые числа\n",
    "int_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    int_sentence = []\n",
    "    for character in sentence:\n",
    "        int_sentence.append(vocab_to_int[character])\n",
    "    int_sentences.append(int_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "kDrDmL1w9G4P"
   },
   "outputs": [],
   "source": [
    "# Длина каждого предложения\n",
    "lengths = []\n",
    "for sentence in int_sentences:\n",
    "    lengths.append(len(sentence))\n",
    "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAi6SPYlfaPA",
    "outputId": "963da146-5d58-49d9-ea2d-da2684d1cb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 63049 to train and test our model.\n"
     ]
    }
   ],
   "source": [
    "# Диапазон длины предложений\n",
    "max_length = 100\n",
    "min_length = 7\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "for sentence in int_sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "\n",
    "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zu1HzGcj9L2y",
    "outputId": "cc636c35-1371-4dad-f9f8-b9b6ab7bd46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 53591\n",
      "Number of testing sentences: 9458\n"
     ]
    }
   ],
   "source": [
    "# Разделение данных на предложения для обучения и тестирования\n",
    "training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
    "\n",
    "print(\"Number of training sentences:\", len(training))\n",
    "print(\"Number of testing sentences:\", len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "TsAo5Zjo9h6K"
   },
   "outputs": [],
   "source": [
    "# Сортировка предложений по длине, чтобы уменьшить отступы\n",
    "# Это позволит модели быстрее обучаться\n",
    "training_sorted = []\n",
    "testing_sorted = []\n",
    "\n",
    "\n",
    "for i in range(min_length, max_length+1):\n",
    "  for sentence in training:\n",
    "    if len(sentence) == i:\n",
    "      training_sorted.append(sentence)\n",
    "  for sentence in testing:\n",
    "    if len(sentence) == i:\n",
    "      testing_sorted.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPesQ-K495tj",
    "outputId": "e5255861-db71-4ddf-afbc-62f33e71ac8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 7, 13, 3, 33, 14, 25] 7\n",
      "[39, 3, 8, 13, 3, 29, 25] 7\n",
      "[41, 14, 21, 3, 22, 24, 25] 7\n",
      "[39, 3, 7, 21, 22, 14, 25] 7\n",
      "[48, 32, 14, 8, 3, 21, 25] 7\n"
     ]
    }
   ],
   "source": [
    "# Проверка результатов сортировки предложений\n",
    "for i in range(5):\n",
    "    print(training_sorted[i], len(training_sorted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "VQkolC5o9ZVs"
   },
   "outputs": [],
   "source": [
    "# Функция аугментации\n",
    "# Перемещение, удаление или добавление символов для создания орфографических ошибок\n",
    "letters = ['а','б','в','г','д','е','ж','з','и','й','к','л',\n",
    "           'м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ь','ъ','ы','э','ю','я',]\n",
    "\n",
    "def noise_maker(sentence, threshold): \n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Большинство символов будут правильными, так как пороговое значение велико\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~33% случайные символы поменяются местами\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # Если последний символ в предложении, он не будет набран\n",
    "                    continue\n",
    "                else:\n",
    "                    # если есть какой-либо другой символ, поменяется порядок на следующий символ\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% в предложение будет добавлена дополнительная строчная буква\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(vocab_to_int[random_letter])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% символ не будет напечатан\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HftYjiZn9dZH",
    "outputId": "8ad6bc1c-2abb-473a-93e5-72277b7819ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 7, 13, 3, 33, 14, 25]\n",
      "[55, 7, 16, 13, 3, 33, 14, 25]\n",
      "\n",
      "[39, 3, 8, 13, 3, 29, 25]\n",
      "[39, 3, 13, 3, 29, 25]\n",
      "\n",
      "[41, 14, 21, 3, 22, 24, 25]\n",
      "[41, 14, 21, 3, 22, 24, 25]\n",
      "\n",
      "[39, 3, 7, 21, 22, 14, 25]\n",
      "[39, 3, 7, 21, 22, 14, 25]\n",
      "\n",
      "[48, 32, 14, 8, 3, 21, 25]\n",
      "[48, 32, 14, 8, 3, 21, 25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверка работы функции аугментации\n",
    "threshold = 0.95\n",
    "for sentence in training_sorted[:5]:\n",
    "    print(sentence)\n",
    "    print(noise_maker(sentence, threshold))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "vjnWGi7bAbyY"
   },
   "outputs": [],
   "source": [
    "# Создайте заполнителей для входных данных в модель\n",
    "def model_inputs():  \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.compat.v1.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs_length = tf.compat.v1.placeholder(tf.int32, (None,), name='inputs_length')\n",
    "    targets_length = tf.compat.v1.placeholder(tf.int32, (None,), name='targets_length')\n",
    "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
    "\n",
    "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "gk38eYgIEi3j"
   },
   "outputs": [],
   "source": [
    "# Удаление идентификатора последнего слова из каждой партии и соединение <GO> с началом каждой партии\n",
    "def process_encoding_input(targets, vocab_to_int, batch_size):   \n",
    "    with tf.name_scope(\"process_encoding\"):\n",
    "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "loUVqkcVEmOh"
   },
   "outputs": [],
   "source": [
    "# encodung layer\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):    \n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
    "                                                              rnn_inputs,\n",
    "                                                              sequence_length,\n",
    "                                                              dtype=tf.float32)\n",
    "\n",
    "            return enc_output, enc_state\n",
    "        \n",
    "        \n",
    "    if direction == 2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                            cell_bw, \n",
    "                                                                            rnn_inputs,\n",
    "                                                                            sequence_length,\n",
    "                                                                            dtype=tf.float32)\n",
    "            # Объединение выходов, так как мы используется двунаправленная RNN\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # Используется только прямое состояние, потому что модель не может использовать оба состояния одновременно\n",
    "            return enc_output, enc_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "y7PdFtYBejzl"
   },
   "outputs": [],
   "source": [
    "# training logits\n",
    "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_target_length):\n",
    "    with tf.name_scope(\"Training_Decoder\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_length)\n",
    "        return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "dp5tH0ZRE14h"
   },
   "outputs": [],
   "source": [
    "# inference logits (вывод)\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_target_length, batch_size):    \n",
    "    with tf.name_scope(\"Inference_Decoder\"):\n",
    "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                output_time_major=False,\n",
    "                                                                impute_finished=True,\n",
    "                                                                maximum_iterations=max_target_length)\n",
    "\n",
    "\n",
    "        return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "q3aOMWEuE2so"
   },
   "outputs": [],
   "source": [
    "## decoding layer and attention\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):  \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.compat.v1.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  inputs_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "      dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                              attn_mech,\n",
    "                                                              rnn_size)\n",
    "    \n",
    "    #initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n",
    "                                                                    # _zero_state_tensors(rnn_size, \n",
    "                                                                    #                     batch_size, \n",
    "                                                                    #                     tf.float32))\n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"decode\"):\n",
    "       training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "       training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer)\n",
    "      \n",
    "        # training_logits = training_decoding_layer(dec_embed_input, \n",
    "        #                                           targets_length, \n",
    "        #                                           dec_cell, \n",
    "        #                                           initial_state,\n",
    "        #                                           output_layer,\n",
    "        #                                           vocab_size, \n",
    "        #                                           max_target_length)\n",
    "\n",
    "\n",
    "       training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                    output_time_major=False,\n",
    "                                                                    impute_finished=True,\n",
    "                                                                    maximum_iterations=max_target_length)\n",
    "\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"decode\", reuse=True):\n",
    "        # inference_logits = inference_decoding_layer(embeddings,  \n",
    "        #                                             vocab_to_int['<GO>'], \n",
    "        #                                             vocab_to_int['<EOS>'],\n",
    "        #                                             dec_cell, \n",
    "        #                                             initial_state, \n",
    "        #                                             output_layer,\n",
    "        #                                             max_target_length,\n",
    "        #                                             batch_size)\n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _, _= tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_length)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "7XP0fQIBE8BA"
   },
   "outputs": [],
   "source": [
    "#seq2seq\n",
    "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):  \n",
    "    enc_embeddings = tf.Variable(tf.random.uniform([vocab_size, embedding_size], -1, 1))\n",
    "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
    "                                           enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        dec_embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        inputs_length, \n",
    "                                                        targets_length, \n",
    "                                                        max_target_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers,\n",
    "                                                        direction)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "8klLHszNFAfi"
   },
   "outputs": [],
   "source": [
    "# предложения дополняются <PAD>, чтобы каждое предложение пакета имело одинаковую длину\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "Zqn994h4FC65"
   },
   "outputs": [],
   "source": [
    "# Пакетные предложения, предложения c ошибками и длина предложений\n",
    "# С каждой эпохой в предложениях будут появляться новые ошибки\n",
    "def get_batches(sentences, batch_size, threshold):    \n",
    "    for batch_i in range(0, len(sentences)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
    "        \n",
    "        sentences_batch_noisy = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
    "            \n",
    "        sentences_batch_eos = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentence.append(vocab_to_int['<EOS>'])\n",
    "            sentences_batch_eos.append(sentence)\n",
    "            \n",
    "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
    "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
    "        \n",
    "        # Длины для параметров _lengths \n",
    "        pad_sentences_lengths = []\n",
    "        for sentence in pad_sentences_batch:\n",
    "            pad_sentences_lengths.append(len(sentence))\n",
    "        \n",
    "        pad_sentences_noisy_lengths = []\n",
    "        for sentence in pad_sentences_noisy_batch:\n",
    "            pad_sentences_noisy_lengths.append(len(sentence))\n",
    "        \n",
    "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "7L1fZrrwFIux"
   },
   "outputs": [],
   "source": [
    "# Параметры по умолчанию\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_layers = 2\n",
    "rnn_size = 512\n",
    "embedding_size = 128\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "threshold = 0.95\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "chlQpIJgFLC4"
   },
   "outputs": [],
   "source": [
    "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    # Входные данные модели    \n",
    "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
    "\n",
    "    # training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      inputs_length,\n",
    "                                                      targets_length,\n",
    "                                                      max_target_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size,\n",
    "                                                      embedding_size,\n",
    "                                                      direction)\n",
    "\n",
    "    # tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "\n",
    "    # weights for sequence_loss\n",
    "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Функция потерь\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
    "                                                targets, \n",
    "                                                masks)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Оптимизатор Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    # Объединение сводок\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
    "                    'predictions', 'merged', 'train_op','optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiQqM9ovrri1",
    "outputId": "3352d3e5-c608-4ed6-f54b-a1d4c1ee216b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Elbrus/My_works/Final_Project/Data/Spell_Checker/cheki\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/Elbrus/My_works/Final_Project/Data/Spell_Checker/cheki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "IP3kZu9rFPyC"
   },
   "outputs": [],
   "source": [
    "# Обучение RNN\n",
    "def train(model, epochs, log_string):   \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Используется для определения того, когда следует рано прекратить тренировку\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        # Какая итерация пакета проходит обучение\n",
    "        iteration = 0\n",
    "        \n",
    "        display_step = 30 #Ход обучения будет отображаться после каждых 30 партий\n",
    "        stop_early = 0 \n",
    "        stop = 3 # Если batch_loss_testing не уменьшается при 3 последовательных проверках, обучение прекращается\n",
    "        per_epoch = 3 # Тестируется модель 3 раза за эпоху\n",
    "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "            \n",
    "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                    get_batches(training_sorted, batch_size, threshold)):\n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.train_op], \n",
    "                                             {model.inputs: input_batch,\n",
    "                                              model.targets: target_batch,\n",
    "                                              model.inputs_length: input_length,\n",
    "                                              model.targets_length: target_length,\n",
    "                                              model.keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "\n",
    "                # Регистрируется ход обучения\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(training_sorted) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                #### Testing ####\n",
    "                if batch_i % testing_check == 0 and batch_i > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                            get_batches(testing_sorted, batch_size, threshold)):\n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: input_batch,\n",
    "                                                      model.targets: target_batch,\n",
    "                                                      model.inputs_length: input_length,\n",
    "                                                      model.targets_length: target_length,\n",
    "                                                      model.keep_prob: 1})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        # Запись ходу тестирования\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch_i + 1\n",
    "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "                    \n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    # Если batch_loss_testing находится на новом минимуме, модель сохраняется\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "PCLr0UUnKQ-O"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Zb7OvhWzNmf"
   },
   "outputs": [],
   "source": [
    "# Проверка подключенного оборудования\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "  local_device_protos = device_lib.list_local_devices()\n",
    "  print(local_device_protos)\n",
    "  return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-rXJ6nYFaKD",
    "outputId": "b60ad479-b576-420f-9f94-22c8df39433a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n",
      "\n",
      "Training Model: kp=0.75,nl=2,th=0.95\n",
      "Epoch   1/100 Batch   30/418 - Loss:  3.005, Seconds: 3.47\n",
      "Epoch   1/100 Batch   60/418 - Loss:  2.120, Seconds: 3.93\n",
      "Epoch   1/100 Batch   90/418 - Loss:  1.492, Seconds: 5.10\n",
      "Epoch   1/100 Batch  120/418 - Loss:  1.000, Seconds: 6.14\n",
      "Testing Loss:  3.645, Seconds: 13.66\n",
      "New Record!\n",
      "Epoch   1/100 Batch  150/418 - Loss:  0.654, Seconds: 7.16\n",
      "Epoch   1/100 Batch  180/418 - Loss:  0.426, Seconds: 8.42\n",
      "Epoch   1/100 Batch  210/418 - Loss:  0.348, Seconds: 9.43\n",
      "Epoch   1/100 Batch  240/418 - Loss:  0.758, Seconds: 10.51\n",
      "Epoch   1/100 Batch  270/418 - Loss:  1.175, Seconds: 12.01\n",
      "Testing Loss:  1.307, Seconds: 15.42\n",
      "New Record!\n",
      "Epoch   1/100 Batch  300/418 - Loss:  0.374, Seconds: 13.87\n",
      "Epoch   1/100 Batch  330/418 - Loss:  0.310, Seconds: 15.22\n",
      "Epoch   1/100 Batch  360/418 - Loss:  0.290, Seconds: 17.06\n",
      "Epoch   1/100 Batch  390/418 - Loss:  0.278, Seconds: 19.16\n",
      "Testing Loss:  2.659, Seconds: 15.97\n",
      "No Improvement.\n",
      "Epoch   2/100 Batch   30/418 - Loss:  0.511, Seconds: 2.55\n",
      "Epoch   2/100 Batch   60/418 - Loss:  0.253, Seconds: 4.06\n",
      "Epoch   2/100 Batch   90/418 - Loss:  0.212, Seconds: 5.28\n",
      "Epoch   2/100 Batch  120/418 - Loss:  0.214, Seconds: 6.21\n",
      "Testing Loss:  2.236, Seconds: 14.15\n",
      "No Improvement.\n",
      "Epoch   2/100 Batch  150/418 - Loss:  0.207, Seconds: 7.33\n",
      "Epoch   2/100 Batch  180/418 - Loss:  0.214, Seconds: 8.43\n",
      "Epoch   2/100 Batch  210/418 - Loss:  0.204, Seconds: 9.59\n",
      "Epoch   2/100 Batch  240/418 - Loss:  0.205, Seconds: 10.82\n",
      "Epoch   2/100 Batch  270/418 - Loss:  0.201, Seconds: 12.25\n",
      "Testing Loss:  2.549, Seconds: 15.86\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели требуемым параметрам настройки\n",
    "# Загрузка весов\n",
    "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
    "with tf.Session() as sess:\n",
    "    # Загрузка модели\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "\n",
    "for keep_probability in [0.75]:\n",
    "    for num_layers in [2]:\n",
    "        for threshold in [0.95]:\n",
    "\n",
    "            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n",
    "                                                    num_layers,\n",
    "                                                    threshold) \n",
    "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "                                learning_rate, embedding_size, direction)\n",
    "            train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "B5Z7P8kzKSIH"
   },
   "outputs": [],
   "source": [
    "def text_to_ints(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0qAJK_Z1hRl",
    "outputId": "da0f14d0-5aed-43f0-ed8b-b95ce2551f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n",
      "\n",
      "Text\n",
      "  Word Ids:    [21, 3, 9, 3, 16]\n",
      "  Input Words: молок\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [21, 3, 9, 3, 16, 3]\n",
      "  Response Words: молоко\n"
     ]
    }
   ],
   "source": [
    "# Проверка правописания\n",
    "text = 'молок'\n",
    "text = text_to_ints(text)\n",
    "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
    "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load saved model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                 model.inputs_length: [len(text)]*batch_size,\n",
    "                                                 model.targets_length: [len(text)+1], \n",
    "                                                 model.keep_prob: [1.0]})[0]\n",
    "\n",
    "# Remove the padding from the generated sentence\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEsmNmaflKqG",
    "outputId": "cb847306-783d-4cca-f9ae-a8fde166d97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n",
      "\n",
      "Text\n",
      "  Word Ids:    [13, 3, 10, 4, 2, 1, 40]\n",
      "  Input Words: тоаврищ\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [13, 3, 4, 10, 2, 1, 40, 1]\n",
      "  Response Words: товарищи\n"
     ]
    }
   ],
   "source": [
    "# Проверка правописания\n",
    "text = 'тоаврищ'\n",
    "text = text_to_ints(text)\n",
    "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
    "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load saved model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint)\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                 model.inputs_length: [len(text)]*batch_size,\n",
    "                                                 model.targets_length: [len(text)+1], \n",
    "                                                 model.keep_prob: [1.0]})[0]\n",
    "\n",
    "# Remove the padding from the generated sentence\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mnemonika_v5_Spell_Checker(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
